---
title: Ollama integration
description: Learn how to use the Aspire Ollama hosting and client integration to host Ollama models using the Ollama container and accessing it via the OllamaSharp client.
---

import { Image } from 'astro:assets';
import InstallPackage from '@components/InstallPackage.astro';
import InstallDotNetPackage from '@components/InstallDotNetPackage.astro';
import { Aside, Badge } from '@astrojs/starlight/components';
import ollamaIcon from '@assets/icons/ollama-icon.png';

<Badge text="â­ Community Toolkit" variant="tip" size="large" />

<Image
  src={ollamaIcon}
  alt="Ollama logo"
  width={100}
  height={100}
  class:list={'float-inline-left icon'}
  data-zoom-off
/>

[Ollama](https://ollama.com) is a powerful, open source language model that can be used to generate text based on a given prompt. The Aspire Ollama integration provides a way to host Ollama models using the [`docker.io/ollama/ollama` container image](https://hub.docker.com/r/ollama/ollama) and access them via the [OllamaSharp](https://www.nuget.org/packages/OllamaSharp) client.

## Hosting integration

The Ollama hosting integration models an Ollama server as the `OllamaResource` type, and provides the ability to add models to the server using the `AddModel` extension method, which represents the model as an `OllamaModelResource` type. To access these types and APIs, install the [ðŸ“¦ CommunityToolkit.Aspire.Hosting.Ollama](https://nuget.org/packages/CommunityToolkit.Aspire.Hosting.Ollama) NuGet package in the AppHost project:

<InstallPackage packageName="CommunityToolkit.Aspire.Hosting.Ollama" />

### Add Ollama resource

In the AppHost project, register and consume the Ollama integration using the `AddOllama` extension method to add the Ollama container to the application builder. You can then add models to the container, which downloads and run when the container starts, using the `AddModel` extension method:

```csharp title="C# â€” AppHost.cs"
var builder = DistributedApplication.CreateBuilder(args);

var ollama = builder.AddOllama("ollama");

var phi35 = ollama.AddModel("phi3.5");

var exampleProject = builder.AddProject<Projects.ExampleProject>()
                            .WithReference(phi35);

builder.Build().Run();
```

Alternatively, if you want to use a model from the [Hugging Face](https://huggingface.co/) model hub, you can use the `AddHuggingFaceModel` extension method:

```csharp
var llama = ollama.AddHuggingFaceModel("llama", "bartowski/Llama-3.2-1B-Instruct-GGUF:IQ4_XS");
```

When Aspire adds a container image to the AppHost, as shown in the preceding example with the `docker.io/ollama/ollama` image, it creates a new Ollama instance on your local machine.

### Download the LLM

When the Ollama container for this integration first spins up, it downloads the configured LLMs. The progress of this download displays in the **State** column for this integration on the Aspire dashboard.

<Aside type="caution">
  Keep the Aspire orchestration app open until the download is complete,
  otherwise the download will be cancelled.
</Aside>

### Cache the LLM

One or more LLMs are downloaded into the container which Ollama is running from, and by default this container is ephemeral. If you need to persist one or more LLMs across container restarts, you need to mount a volume into the container using the `WithDataVolume` method:

```csharp
var ollama = builder.AddOllama("ollama")
                    .WithDataVolume();

var llama = ollama.AddModel("llama3");
```

### Use GPUs when available

One or more LLMs are downloaded into the container which Ollama is running from, and by default this container runs on CPU. If you need to run the container with GPU support, you can enable it using the `WithGPUSupport()` extension method.

**Nvidia:**

```csharp
var ollama = builder.AddOllama("ollama")
                    .AddModel("llama3")
                    .WithGPUSupport();
```

**AMD:**

```csharp
var ollama = builder.AddOllama("ollama")
                    .AddModel("llama3")
                    .WithGPUSupport(OllamaGpuVendor.AMD);
```

For more information, see [GPU support in Docker Desktop](https://docs.docker.com/desktop/gpu/) and [GPU support in Podman](https://github.com/containers/podman/issues/19005).

### Hosting integration health checks

The Ollama hosting integration automatically adds a health check for the Ollama server and model resources. For the Ollama server, a health check is added to verify that the Ollama server is running and that a connection can be established to it. For the Ollama model resources, a health check is added to verify that the model is running and that the model is available, meaning the resource will be marked as unhealthy until the model has been downloaded.

### Open WebUI support

The Ollama integration also provided support for running [Open WebUI](https://openwebui.com/) and having it communicate with the Ollama container:

```csharp
var ollama = builder.AddOllama("ollama")
                    .AddModel("llama3")
                    .WithOpenWebUI();
```

## Client integration

To get started with the Aspire OllamaSharp integration, install the [ðŸ“¦ CommunityToolkit.Aspire.OllamaSharp](https://nuget.org/packages/CommunityToolkit.Aspire.OllamaSharp) NuGet package in the client-consuming project, that is, the project for the application that uses the Ollama client:

<InstallDotNetPackage packageName="CommunityToolkit.Aspire.OllamaSharp" />

### Add Ollama client API

In the `Program.cs` file of your client-consuming project, call the `AddOllamaClientApi` extension to register an `IOllamaClientApi` for use via the dependency injection container. If the resource provided in the AppHost, and referenced in the client-consuming project, is an `OllamaModelResource`, then the `AddOllamaClientApi` method will register the model as the default model for the `IOllamaClientApi`:

```csharp
builder.AddOllamaClientApi("llama3");
```

After adding `IOllamaClientApi` to the builder, you can get the `IOllamaClientApi` instance using dependency injection. For example, to retrieve your context object from service:

```csharp
public class ExampleService(IOllamaClientApi ollama)
{
    // Use ollama...
}
```

### Add keyed Ollama client API

There might be situations where you want to register multiple `IOllamaClientApi` instances with different connection names. To register keyed Ollama clients, call the `AddKeyedOllamaClientApi` method:

```csharp
builder.AddKeyedOllamaClientApi(name: "chat");
builder.AddKeyedOllamaClientApi(name: "embeddings");
```

Then you can retrieve the `IOllamaClientApi` instances using dependency injection. For example, to retrieve the connection from an example service:

```csharp
public class ExampleService(
    [FromKeyedServices("chat")] IOllamaClientApi chatOllama,
    [FromKeyedServices("embeddings")] IOllamaClientApi embeddingsOllama)
{
    // Use ollama...
}
```

### Configuration

The Ollama client integration provides multiple configuration approaches and options to meet the requirements and conventions of your project.

#### Use a connection string

When using a connection string from the `ConnectionStrings` configuration section, you can provide the name of the connection string when calling the `AddOllamaClientApi` method:

```csharp
builder.AddOllamaClientApi("llama");
```

Then the connection string will be retrieved from the `ConnectionStrings` configuration section:

```json
{
  "ConnectionStrings": {
    "llama": "Endpoint=http//localhost:1234;Model=llama3"
  }
}
```

### Integration with Microsoft.Extensions.AI

The [ðŸ“¦ Microsoft.Extensions.AI](https://www.nuget.org/packages/Microsoft.Extensions.AI) NuGet package provides an abstraction over the Ollama client API, using generic interfaces. OllamaSharp supports these interfaces, and they can be registered by chaining either the `IChatClient` or `IEmbeddingGenerator<string, Embedding<float>>` registration methods to the `AddOllamaClientApi` method.

To register an `IChatClient`, chain the `AddChatClient` method to the `AddOllamaClientApi` method:

```csharp
builder.AddOllamaClientApi("llama")
       .AddChatClient();
```

Similarly, to register an `IEmbeddingGenerator`, chain the `AddEmbeddingGenerator` method:

```csharp
builder.AddOllamaClientApi("llama")
       .AddEmbeddingGenerator();
```

After adding `IChatClient` to the builder, you can get the `IChatClient` instance using dependency injection. For example, to retrieve your context object from service:

```csharp
public class ExampleService(IChatClient chatClient)
{
    // Use chat client...
}
```

### Add keyed Microsoft.Extensions.AI clients

There might be situations where you want to register multiple AI client instances with different connection names. To register keyed AI clients, use the keyed versions of the registration methods:

```csharp
builder.AddOllamaClientApi("chat")
       .AddKeyedChatClient("chat");
builder.AddOllamaClientApi("embeddings")
       .AddKeyedEmbeddingGenerator("embeddings");
```

Then you can retrieve the AI client instances using dependency injection. For example, to retrieve the clients from an example service:

```csharp
public class ExampleService(
    [FromKeyedServices("chat")] IChatClient chatClient,
    [FromKeyedServices("embeddings")] IEmbeddingGenerator<string, Embedding<float>> embeddingGenerator)
{
    // Use AI clients...
}
```

## See also

- [Ollama](https://ollama.com)
- [Open WebUI](https://openwebui.com)
- [Aspire Community Toolkit GitHub repo](https://github.com/CommunityToolkit/Aspire)
- [OllamaSharp](https://github.com/awaescher/OllamaSharp)
- [Microsoft.Extensions.AI](https://devblogs.microsoft.com/dotnet/introducing-microsoft-extensions-ai-preview/)
